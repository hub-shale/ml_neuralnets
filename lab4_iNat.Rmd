---
title: "iNaturalist"
author: "Shale"
date: "2/14/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)

options(scipen = 100)
```

# Data Setup

```{r}
librarian::shelf(
  digest, dplyr, DT, glue, purrr, readr, stringr, tidyr, here, keras, tensorflow, reticulate)

# path to folder containing species directories of images
dir_src  <- "/courses/EDS232/inaturalist-2021/train_mini"
dir_dest <- "~/inat"
dir.create(dir_dest, showWarnings = F)

# get list of directories, one per species (n = 10,000 species)
dirs_spp <- list.dirs(dir_src, recursive = F, full.names = T)
n_spp <- length(dirs_spp)

# set seed (for reproducible results) 
# just before sampling (otherwise get different results)
# based on your username (unique amongst class)
Sys.info()[["user"]] %>% 
  digest::digest2int() %>% 
  set.seed()
i10 <- sample(1:n_spp, 10)

# show the 10 indices sampled of the 10,000 possible 
i10

```

```{r}
# show the 10 species directory names
basename(dirs_spp)[i10]
```
```{r}
i2 <- i10[1:2]
basename(dirs_spp)[i2]
```

```{r, eval=FALSE}
# setup data frame with source (src) and destination (dest) paths to images
idata <- tibble(
  set     = c(rep("spp2", 2), rep("spp10", 10)),
  dir_sp  = c(dirs_spp[i2], dirs_spp[i10]),
  tbl_img = map(dir_sp, function(dir_sp){
    tibble(
      src_img = list.files(dir_sp, full.names = T),
      subset  = c(rep("train", 30), rep("validation", 10), rep("test", 10))) })) %>% 
  unnest(tbl_img) %>% 
  mutate(
    sp       = basename(dir_sp),
    img      = basename(src_img),
    dest_img = glue("{dir_dest}/{set}/{subset}/{sp}/{img}"))

# show source and destination for first 10 rows of tibble
# idata %>% 
#   select(src_img, dest_img)

idata %>% 
  pwalk(function(src_img, dest_img, ...){
    dir.create(dirname(dest_img), recursive = T, showWarnings = F)
    file.copy(src_img, dest_img) })
```

Tried a thing:

```{r, eval=FALSE}
# filepath labels
train2_dir = file.path(dir_dest, "spp2/train")
test2_dir = file.path(dir_dest, "spp2/test")
validation2_dir = file.path(dir_dest, "spp2/validation")
train10_dir = file.path(dir_dest, "spp10/train")
test10_dir = file.path(dir_dest, "spp10/test")
validation10_dir = file.path(dir_dest, "spp10/validation")

# Resize images for binary classification
datagen_resizr <- image_data_generator(rescale = 1/255)

tensor_gen <- function(img_subset) {
  
  img_dir2 = paste0(img_subset + "2_dir")
  img_dir10 = paste0(img_subset + "10_dir")
    
    flow_images_from_directory(
  img_dir2,
  datagen_resizr,
  target_size = c(150, 150),
  batch_size = 10,
  class_mode = "binary"
  )
    
    generator10 <- flow_images_from_directory(
  img_dir10,
  datagen_resizr,
  target_size = c(150, 150),
  batch_size = 10,
  class_mode = "binary"
  )
}
```

Manually:

```{r}
# filepath labels
train_dir = here("inat/spp2/train")
test_dir = here("inat/spp2/test")
validation_dir = here("inat/spp2/validation")
train10_dir = here("inat/spp10/train")
test10_dir = here("inat/spp10/test")
validation10_dir = here("inat/spp10/validation")

# Resize images for binary classification
datagen <- image_data_generator(rescale = 1/255)

# Avoid overfitting
train_datagen <- image_data_generator(
  rescale = 1/255,
  rotation_range = 40,
  width_shift_range = 0.2,
  height_shift_range = 0.2,
  shear_range = 0.2,
  zoom_range = 0.2,
  horizontal_flip = TRUE,
  fill_mode = "nearest"
)

train_generator <- flow_images_from_directory(
  train_dir,
  train_datagen,
  target_size = c(150, 150),
  batch_size = 30,
  class_mode = "binary"
)

test_generator <- flow_images_from_directory(
  test_dir,
  datagen,
  target_size = c(150, 150),
  batch_size = 10,
  class_mode = "binary"
)

val_generator <- flow_images_from_directory(
  validation_dir,
  datagen,
  target_size = c(150, 150),
  batch_size = 10,
  class_mode = "binary"
)
```

# Binary Neural Net

```{r}
nn_model <- keras_model_sequential() %>% 
  layer_dense(units = 16, activation = "relu", input_shape = c(150,150,3)) %>% 
  layer_dense(units = 16, activation = "relu") %>% 
  layer_flatten() %>%
  layer_dense(units =  1, activation = "sigmoid")
```

```{r}
nn_model %>% compile(
  optimizer = "rmsprop",
  loss      = "binary_crossentropy",
  metrics   = c("accuracy"))
```

```{r}
nn_history <- nn_model %>% fit(
  train_generator,
  steps_per_epoch = 1,
  epochs = 20,
  validation_data = val_generator,
  validation_steps = 1,
  verbose = 1
)
```

```{r}
plot(nn_history)

# nn_model %>% predict(test_generator)

nn_model %>% evaluate(test_generator, steps = 10)
```

# Binary Convolutional Neural Net

```{r}
conv_model <- keras_model_sequential() %>%
layer_conv_2d(filters = 32, kernel_size = c(3, 3), activation = "relu",
input_shape = c(150, 150, 3)) %>%
layer_max_pooling_2d(pool_size = c(2, 2)) %>%
layer_conv_2d(filters = 64, kernel_size = c(3, 3), activation = "relu") %>% layer_max_pooling_2d(pool_size = c(2, 2)) %>%
layer_conv_2d(filters = 128, kernel_size = c(3, 3), activation = "relu") %>% layer_max_pooling_2d(pool_size = c(2, 2)) %>%
layer_conv_2d(filters = 128, kernel_size = c(3, 3), activation = "relu") %>% layer_max_pooling_2d(pool_size = c(2, 2)) %>%
layer_flatten() %>%
layer_dense(units = 512, activation = "relu") %>%
layer_dense(units = 1, activation = "sigmoid")
```

```{r}
conv_model %>% compile(
  loss = "binary_crossentropy",
  optimizer = "rmsprop",
  metrics = c("acc")
)
```

```{r}
c_history <- conv_model %>% fit(
  train_generator,
  steps_per_epoch = 2,
  epochs = 20,
  validation_data = val_generator,
  validation_steps = 2
)
```

```{r}
plot(c_history)

# conv_model %>% predict(test_generator)

conv_model %>% evaluate(test_generator, steps = 10)
```

## Comparison of NN/CNN for Binary Classification:
**For the binary classification problem, I found that the standard neural net and convolutional neural net typically performed quite similarly on the testing data, with loss values around 0.3-0.5 and accuracy usually between 80-90%. Because the models learn slightly differently each time, the exact numbers vary. They are both calculated after 20 epochs (reduced from 30 to save on processing time because accuracy was not any higher with 30 vs 20 epochs). **

# Multiclass Neural Net

```{r}
train10_generator <- flow_images_from_directory(
  train10_dir,
  train_datagen,
  target_size = c(150, 150),
  batch_size = 20,
  class_mode = "categorical"
)

test10_generator <- flow_images_from_directory(
  test10_dir,
  datagen,
  target_size = c(150, 150),
  batch_size = 10,
  class_mode = "categorical"
)

val10_generator <- flow_images_from_directory(
  validation10_dir,
  datagen,
  target_size = c(150, 150),
  batch_size = 10,
  class_mode = "categorical"
)
```

```{r}
model10 <- keras_model_sequential() %>% 
  layer_dense(units = 64, activation = "relu", input_shape = c(150,150,3)) %>% 
  layer_dense(units = 64, activation = "relu") %>% 
  layer_flatten() %>%
  layer_dense(units = 64, activation = "relu") %>% 
  layer_dense(units = 10, activation = "softmax")
```

```{r}
model10 %>% compile(
  optimizer = "rmsprop",
  loss = "categorical_crossentropy",
  metrics = c("acc")
)
```

```{r}
history10 <- fit(model10,
  train10_generator,
  steps_per_epoch = 10,
  epochs = 20,
  validation_data = val10_generator,
  validation_steps = 10
)
```

```{r}
plot(history10)

# model10 %>% predict(test10_generator) %>% DT::datatable(options = list(pageLength = 20))

model10 %>% evaluate(test10_generator, steps = 100)

loss_50e = 2.25
acc_50e = .52
```

# Multiclass Convolutional Neural Net

```{r}
conv10_model <- keras_model_sequential() %>%
layer_conv_2d(filters = 32, kernel_size = c(3, 3), activation = "relu",
input_shape = c(150, 150, 3)) %>%
layer_max_pooling_2d(pool_size = c(2, 2)) %>%
layer_conv_2d(filters = 64, kernel_size = c(3, 3), activation = "relu") %>% layer_max_pooling_2d(pool_size = c(2, 2)) %>%
layer_conv_2d(filters = 128, kernel_size = c(3, 3), activation = "relu") %>% layer_max_pooling_2d(pool_size = c(2, 2)) %>%
layer_conv_2d(filters = 128, kernel_size = c(3, 3), activation = "relu") %>% layer_max_pooling_2d(pool_size = c(2, 2)) %>%
layer_flatten() %>%
layer_dense(units = 512, activation = "relu") %>%
layer_dense(units = 10, activation = "softmax")
```

```{r}
conv10_model %>% compile(
  loss = "categorical_crossentropy",
  optimizer = "rmsprop",
  metrics = c("acc")
)
```

```{r}
c10_history <- conv10_model %>% fit(
  train10_generator,
  steps_per_epoch = 10,
  epochs = 20,
  validation_data = val10_generator,
  validation_steps = 10
)
```

```{r}
plot(c10_history)

# conv10_model %>% predict(test10_generator) %>% DT::datatable(options = list(pageLength = 20))

conv10_model %>% evaluate(test10_generator, steps = 100)

```

## Comparison of NN/CNN for Categorical (n=10) Classification
**For the multiclass classification problem, the standard neural net typically had a testing accuracy above 50% while the convolutional neural net usually performed slightly worse, with testing accuracy below 50%. Both loss values hovered around 1.5. Overall, the lack of accuracy in these models can be ascribed to the lack of training data (30 images were used instead of thousands, which is more typical). This means that the effectiveness of parameter tuning is severely limited: for example, after training a model with 50 epochs, the multiclass convolutional neural network still only achieved an accuracy of 51%.**
